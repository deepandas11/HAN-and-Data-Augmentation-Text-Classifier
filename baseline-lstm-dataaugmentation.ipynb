{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3430e80a899a2ede26052d76ab0de5537b8ea7ca"
   },
   "source": [
    "# Baseline LSTM Model using GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import sys,os, re, csv, codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras import initializers, regularizers, constraints, optimizers,layers\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2540ba783963765499ab1f39bfb6418be816a9a3"
   },
   "source": [
    "Embedding File : Wikipedia GloVe dataset with 200-Dimensional representation -> embed_size\n",
    "\n",
    "Maximum number of unique words to use: 20000 -> max_features\n",
    "\n",
    "Maximum number of words to use in a comment: 100 -> maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_file_en= '../input/jigsaw-toxic-comment-classification-challenge/train.csv'\n",
    "train_file_de= '../input/toxic-comments-french-spanish-german-train/train_de.csv'\n",
    "train_file_fr= '../input/toxic-comments-french-spanish-german-train/train_fr.csv'\n",
    "train_file_es= '../input/toxic-comments-french-spanish-german-train/train_es.csv'\n",
    "\n",
    "test_file= '../input/jigsaw-toxic-comment-classification-challenge/test.csv'\n",
    "test_label_file = '../input/jigsaw'\n",
    "\n",
    "embedding_file = '../input/glove6b200d/glove.6B.200d.txt'\n",
    "\n",
    "train_en = pd.read_csv(train_file_en)\n",
    "train_es = pd.read_csv(train_file_es)\n",
    "train_fr = pd.read_csv(train_file_fr)\n",
    "train_de = pd.read_csv(train_file_de)\n",
    "\n",
    "train = train_en.append(train_es.append(train_de.append(train_fr, ignore_index= True), ignore_index=True), ignore_index=True)\n",
    "test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eacf9fd9ca1b48abb8c90c6b44b1c172244c867f"
   },
   "outputs": [],
   "source": [
    "embed_size = 200\n",
    "max_features = 200000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a81d243428f3e4ee739f6960adf7649204b16c6c"
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "list_classes = [\"toxic\",\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_t = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a134e71a88ed0b1a5f83021cb582d4c6931017df"
   },
   "outputs": [],
   "source": [
    "np.shape(y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e806d2e08f97f554fc6d3cdbf27ea27506b3b08"
   },
   "source": [
    "# Preparing the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2254931e0f1220d02072c1169f5a6310e43be320"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1dd62c745cef91eb0a938f29c951696a1d3bdc89"
   },
   "outputs": [],
   "source": [
    "np.shape(X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "660fb0a6582049ccacf3eb8e0c8bfc0d89c234e7"
   },
   "source": [
    "# GloVe Word Embeddings\n",
    "\n",
    "We will be using GloVe embeddings, which you can read about here. GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics.\n",
    "\n",
    "Specifically, we will use the **200-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia.** \n",
    "\n",
    "## Preparing Embedding Layer\n",
    "\n",
    "Returns a 20-dimensional vector space coordinate for each word in the embedding, i.e. 400k words each have a 200-d representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28f6b5c55c144e97ea3cba4be7d250774b5bfa04"
   },
   "outputs": [],
   "source": [
    "embeddings_index={}\n",
    "\n",
    "f = open(embedding_file)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Found %s word vectors.'%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7dac7890c32661fa3f67e18d128ac604f0cdf971"
   },
   "source": [
    "## Preparing Embedding Matrix\n",
    "\n",
    "Words that are not in GloVe are randomly initialized\n",
    "Creating an embedding matrix for 20000 words where each word has a 200 dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a0259afb1f425bb306070ac3fd74c72d3026292"
   },
   "outputs": [],
   "source": [
    "all_embeddings = np.stack(embeddings_index.values())\n",
    "\n",
    "emb_mean= all_embeddings.mean()\n",
    "emb_stddev = all_embeddings.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03458e54eeb784188cf0d5ef13a0f0ca90eab1c7"
   },
   "outputs": [],
   "source": [
    "np.shape(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5600b6b3398d2cf592698a2b2345dd20fdf4799f"
   },
   "outputs": [],
   "source": [
    "nb_words = min(max_features, len(word_index)+1)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_stddev, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i>=max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48963a214c6a4ac71ecc45495348e974801c4933"
   },
   "outputs": [],
   "source": [
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3ec14c49ed39703f69517750aabf1bacfd0f95b"
   },
   "source": [
    "## Model\n",
    "\n",
    "Simple bidirectional LSTM with two fully connected layers and adding some dropout is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52eb4bd20a5cca3b3864f36de89c18e015c7c2ab"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights = [embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "128bb27e4071575c929e71326994d25e3eced5af"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_t, y_t, batch_size=50, epochs=10, validation_split=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "397b8ccff6b59c7a6b2ff8f27b1f372da9600688"
   },
   "outputs": [],
   "source": [
    "y_test = model.predict([X_te],batch_size = 1024, verbose=1)\n",
    "sample_submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0d4208bf597a037530b2dac0698b0edc8bb1946"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7e8daf485563627f05a1b2374116fc84291880d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
