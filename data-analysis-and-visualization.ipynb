{"cells":[{"metadata":{"_uuid":"1a3522ad964c7864311c24618adcc85fc1da2ac1"},"cell_type":"markdown","source":"# Data Visualization\n\n This notebook aims to reveal meaningful insights by applying various visualization and feature engineering techniques on the Toxic Comments Challenge dataset. \n \n** Visualizations/Analysis : \n** \n 1. Categorical spread of overall data\n 2. Categorical spread of toxic data\n 3. Feature Importance\n 4. Adversarial Validation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib_venn as venn\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n#Dataset\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Basic Tabular description of the Training dataset used here\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"371767c05dbe53df2729ba054bfae93966b0f034"},"cell_type":"code","source":"#Inserting a num_words column into dataframe\n\ntrain_df[\"num_words\"] = train_df[\"comment_text\"].apply(lambda x:len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"comment_text\"].apply(lambda x:len(str(x).split()))\n\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34b9c04440accea74ff32a569bd28d50148ed885"},"cell_type":"code","source":"#Checking on the first few rows of the dataframe\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"899f8b09e0b9c8014c4bf4ce07f1b35a6cca3ec0"},"cell_type":"markdown","source":"## Distribution of data "},{"metadata":{"trusted":true,"_uuid":"0e52b62e5697ec16ff96eb85a8ec59baa1df9dcb"},"cell_type":"code","source":"#Code block to visualize the number of total toxic comments of any category and non-toxic comments\ntrain_df[\"toxicity_score\"] = train_df.iloc[:,2:-2].sum(axis=1)\ntypes = [\"toxic\", \"non-toxic\"]\ntoxic_count = len(train_df[train_df[\"toxicity_score\"]>0])\nnontoxic_count = train_df.shape[0]-toxic_count\ncomments_count = [toxic_count, nontoxic_count]\n\nplt.figure(figsize=(5,3))\nax = sns.barplot(types, comments_count)\nplt.xlabel('Categories')\nplt.ylabel('Counts')\nplt.title('Categorical count - I')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"325a0026b62f7a2cdc16edd9da97aa81066488fd"},"cell_type":"code","source":"#Code block to visualize the populations of the different categories of toxicity\n\ncounts = train_df.iloc[:,2:-2].sum(axis=0)\ncounts_values = counts.values\ncategories = counts.index.values\n\nplt.figure(figsize=(9,3))\nax = sns.barplot(categories, counts_values, alpha = 0.8)\nplt.xlabel('categories')\nplt.ylabel('Counts')\nplt.title('Categorical count - II')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e6e7be4775d6025e822f9dc54b5edbf12a63bbc"},"cell_type":"markdown","source":"We see that a very little section of the entire dataset is actually toxic and that further, the toxic comments are not evenly spread out across the several categories.\n\n## Cleaning\n"},{"metadata":{"trusted":true,"_uuid":"b92b36ceb425531cd3ed5d65ab0f53aa024b56cc"},"cell_type":"code","source":"import re, string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\ntokenizer=TweetTokenizer()\n\neng_stopwords = set(stopwords.words(\"english\"))\n\nmerge = pd.concat([train_df, test_df])\ncorpus = merge['comment_text']\ndef clean(text):\n    text = text.lower()\n    text = re.sub(\"\\\\n\",\"\",text) #Removes newlines\n    text=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",text) #Removes IP addresses\n    text=re.sub(\"\\[\\[.*\\]\",\"\",text) #Removes usernames\n    words=tokenizer.tokenize(text) #Tokenization\n    words = [w for w in words if not w in eng_stopwords]\n    clean_text = \" \".join(words)\n    return(clean_text)\n\nclean_corpus=corpus.apply(lambda x:clean(x)) #Clean corpus of data for both training and testing combined","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5517dac9d1009dad0e925168154966c33cdb7c2f"},"cell_type":"markdown","source":"## Feature Visualization - TFIDF"},{"metadata":{"trusted":true,"_uuid":"d37896880326aec5b9a58be5786a50601f52ec47"},"cell_type":"code","source":"tf_idf = TfidfVectorizer(min_df = 20, max_features = 100000,\n                         analyzer = 'word', ngram_range=(1,1), \n                         use_idf=1,smooth_idf=1,sublinear_tf=1,\n                        stop_words = 'english', strip_accents = 'unicode')\ntfidf_fit = tf_idf.fit(clean_corpus)\nfeatures = np.array(tfidf_fit.get_feature_names()) #Get all the features/words that have a corresponding TF-IDF score\ntrain_unigrams =  tfidf_fit.transform(clean_corpus.iloc[:train_df.shape[0]])\ntest_unigrams = tfidf_fit.transform(clean_corpus.iloc[train_df.shape[0]:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9452b8985ec2ebee658de23bc8e2ae7b1ab8381c"},"cell_type":"code","source":"#Top n tf-idf values in a row \n \ndef top_tfidf_features(row, features, top_n = 10):\n    topn_ids = np.argsort(row)[::-1][:top_n] # Get indices of the highest top_n features\n    top_feats = [(features[i], row[i]) for i in topn_ids] #Get the corresponding feature names\n    df = pd.DataFrame(top_feats) \n    df.columns = ['feature', 'tfidf']\n    return df\n\n#Top tfidf features in specific comment(matrix row)\n\ndef top_feats_in_comment(Xtr, features, row_id, top_n=10):\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_features(row, features, top_n)\n\n#Top n features that are most important across comments in rows \ndef top_mean_features(Xtr, features, grp_ids, min_tfidf=0.1, top_n=10):\n    D = Xtr[grp_ids].toarray() #Get an array out of the unigrams\n    D[D < min_tfidf] = 0 #Threshold for minimum tf-idf score\n    tfidf_means = np.mean(D, axis=0) #Mean of all tf-idf values\n    return top_tfidf_features(tfidf_means, features, top_n) \n\n#List of dataframes that holds top n features and the tfidf value calculated across documents/comments of the same label.\ndef top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=10):\n    dfs = []\n    cols=train_tags.columns\n    for col in cols:\n        ids = train_tags.index[train_tags[col]==1]\n        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        #feats_df.label = label\n        dfs.append(feats_df)\n    return dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4ecf2ef90cb7ae98b506ccbc59b7e90290f46ca"},"cell_type":"code","source":"train_tags = train_df.iloc[:,2:-2] #All the scores for different categories\ntfidf_top_n_per_class = top_feats_by_class(train_unigrams, features) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22047d082272bc9eed88353ffc60588c529cb8f9"},"cell_type":"code","source":"#List top tf-idf features for the nth comment in the training set\ntop_tfidf_features(np.squeeze(train_unigrams[54].toarray()),features,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a79f9431795b1a64a6c1bd414cfd939f304bd13"},"cell_type":"code","source":"import matplotlib.gridspec as gridspec \nplt.figure(figsize=(16,22))\nplt.suptitle(\"TF_IDF Top words per class(unigrams)\",fontsize=20)\ncolor = sns.color_palette()\n\ngridspec.GridSpec(3,2)\nplt.subplot2grid((3,2),(0,0))\nsns.barplot(tfidf_top_n_per_class[0].feature.iloc[0:9],tfidf_top_n_per_class[0].tfidf.iloc[0:9],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.subplot2grid((3,2),(0,1))\nsns.barplot(tfidf_top_n_per_class[1].feature.iloc[0:9],tfidf_top_n_per_class[1].tfidf.iloc[0:9],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(1,0))\nsns.barplot(tfidf_top_n_per_class[2].feature.iloc[0:9],tfidf_top_n_per_class[2].tfidf.iloc[0:9],color=color[2])\nplt.title(\"class : Obscene\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(1,1))\nsns.barplot(tfidf_top_n_per_class[3].feature.iloc[0:9],tfidf_top_n_per_class[3].tfidf.iloc[0:9],color=color[3])\nplt.title(\"class : Threat\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,0))\nsns.barplot(tfidf_top_n_per_class[4].feature.iloc[0:9],tfidf_top_n_per_class[4].tfidf.iloc[0:9],color=color[4])\nplt.title(\"class : Insult\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,1))\nsns.barplot(tfidf_top_n_per_class[5].feature.iloc[0:9],tfidf_top_n_per_class[5].tfidf.iloc[0:9],color=color[5])\nplt.title(\"class : Identity hate\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158ce795c904b2e313a33e964ce4cb811f520bb0"},"cell_type":"markdown","source":"## Adversarial Validation\n\n\nWe will be predicting if an individual comment is from the training set or the testing set. It is the idea to check the degree of similarity between training and test datasets in terms of feature distributions. If they are difficult to distinguish, the distribution is probably similar and the normal validation techniques should work"},{"metadata":{"trusted":true,"_uuid":"8a746c6843c593f226b576bcb3f468d6de437b41"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n#Assigning Value 1 to Training Set and 0 to Testing Set\ntrain['is_train']=1\ntest['is_train']=0\nmerge = pd.concat([train, test])\nmerge.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'id'], axis=1, inplace=True)\nmerge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71e47a76930603c7919f8c06a931d8589d173545"},"cell_type":"code","source":"X_train, X_test, y_train, y_valid = train_test_split(merge, merge['is_train'], test_size=0.2, random_state=14) \nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06ed9cdf9d158e2cdfe237147a4bf0b54e50d748"},"cell_type":"code","source":"tfidf_vec = TfidfVectorizer(ngram_range=(1,4), #4-gram\n                            analyzer = 'word', \n                            stop_words=None,\n                            max_features = 20000,\n                            binary=True)\ntrain_tfidf = tfidf_vec.fit_transform(X_train['comment_text'])\ntest_tfidf = tfidf_vec.transform(X_test['comment_text'])\nprint(train_tfidf.shape)\nprint(test_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70edf676882c3ecedc718f7dbba7f283a90824a9"},"cell_type":"code","source":"model = LogisticRegression(solver='newton-cg') #Simple classification\nmodel.fit(train_tfidf, y_train)\npred_test_y = model.predict_proba(test_tfidf)[:, 1]\nprint('AUC of guessing test: {}'.format(roc_auc_score(y_valid, pred_test_y)))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"585ae3255d0e2094ef38ec842149472daae53fce"},"cell_type":"code","source":"import eli5\n\nX_test['pred'] = pred_test_y\nX_test.head()\n\neli5.show_weights(model, vec = tfidf_vec, top=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d615d7331a44634b93ce07b5a9452f0bc4c6698f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}