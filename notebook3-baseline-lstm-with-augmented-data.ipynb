{
  "cells": [
    {
      "metadata": {
        "_uuid": "3430e80a899a2ede26052d76ab0de5537b8ea7ca"
      },
      "cell_type": "markdown",
      "source": "# Baseline LSTM Model using GloVe Embeddings"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys,os, re, csv, codecs\nimport numpy as np\nimport pandas as pd\nimport keras\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.models import Model\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras import initializers, regularizers, constraints, optimizers,layers\nfrom keras.callbacks import EarlyStopping\nfrom time import time",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2540ba783963765499ab1f39bfb6418be816a9a3"
      },
      "cell_type": "markdown",
      "source": "Embedding File : Wikipedia GloVe dataset with 200-Dimensional representation -> embed_size\n\nMaximum number of unique words to use: 20000 -> max_features\n\nMaximum number of words to use in a comment: 100 -> maxlen"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#training and testing files with the data augmentation sets \ntrain_file_en= '../input/jigsaw-toxic-comment-classification-challenge/train.csv'\ntrain_file_de= '../input/toxic-comments-french-spanish-german-train/train_de.csv'\ntrain_file_fr= '../input/toxic-comments-french-spanish-german-train/train_fr.csv'\ntrain_file_es= '../input/toxic-comments-french-spanish-german-train/train_es.csv'\n\ntest_file= '../input/jigsaw-toxic-comment-classification-challenge/test.csv'\ntest_label_file = '../input/jigsaw'\n\n#Using the GloVe embedding file\nembedding_file = '../input/glove6b200d/glove.6B.200d.txt'\n\ntrain_en = pd.read_csv(train_file_en)\ntrain_es = pd.read_csv(train_file_es)\ntrain_fr = pd.read_csv(train_file_fr)\ntrain_de = pd.read_csv(train_file_de)\n\n#Augmented Dataset\ntrain = train_en.append(train_es.append(train_de, ignore_index=True), ignore_index=True)\ntest = pd.read_csv(test_file)\n\n#Only English\ntrain1 = train_en",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eacf9fd9ca1b48abb8c90c6b44b1c172244c867f"
      },
      "cell_type": "code",
      "source": "embed_size = 200 #Embedding dimension\nmax_features = 200000 #Maximum number of words to keep based on frequency\nmaxlen = 100 #Maximum Length of comments",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a81d243428f3e4ee739f6960adf7649204b16c6c"
      },
      "cell_type": "code",
      "source": "#Extracting the comments and removing the Empty comments\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_sentences_train1 = train1[\"comment_text\"].fillna(\"_na_\").values\n\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n\n#Categories\nlist_classes = [\"toxic\",\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\n#Category Tags\ny_t = train[list_classes].values\ny_t1 = train1[list_classes].values\n\n#np.shape(y_t1)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3e806d2e08f97f554fc6d3cdbf27ea27506b3b08"
      },
      "cell_type": "markdown",
      "source": "# Preparing the Text Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2254931e0f1220d02072c1169f5a6310e43be320"
      },
      "cell_type": "code",
      "source": "#Using Keras' Tokenizer function on augmented dataset\n#Vectorize a text corpus, by turning each text into either a sequence of integers \ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train)) \n#Takes the 200000 max frequency words in the dataset and creates tokens out of them\n\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n\n#Word Index holds the index integer values for all the words based on the Tokenizer aas a dictionary\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' %len(word_index))",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found 246556 unique tokens.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6bf0be2bc9848eadfabac7811f80a281846cd7e"
      },
      "cell_type": "code",
      "source": "#Word Index Dictionary where each \nword_index",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "{'the': 1,\n 'to': 2,\n 'of': 3,\n 'and': 4,\n 'a': 5,\n 'i': 6,\n 'you': 7,\n 'is': 8,\n 'that': 9,\n 'in': 10,\n 'not': 11,\n 'it': 12,\n 'for': 13,\n 'this': 14,\n 'on': 15,\n 'be': 16,\n 'have': 17,\n 'are': 18,\n 'as': 19,\n 'your': 20,\n 'if': 21,\n 'with': 22,\n 'do': 23,\n 'article': 24,\n 'was': 25,\n 'or': 26,\n 'but': 27,\n 'wikipedia': 28,\n 'an': 29,\n 'can': 30,\n 'page': 31,\n 'my': 32,\n 'from': 33,\n 'by': 34,\n 'about': 35,\n 'me': 36,\n 'will': 37,\n 'what': 38,\n 'so': 39,\n 'has': 40,\n 'there': 41,\n 'at': 42,\n 'they': 43,\n 'he': 44,\n 'would': 45,\n 'all': 46,\n 'no': 47,\n 'talk': 48,\n 'like': 49,\n 'should': 50,\n 'who': 51,\n 'we': 52,\n 'please': 53,\n 'one': 54,\n 'just': 55,\n 'more': 56,\n 'other': 57,\n 'which': 58,\n 'think': 59,\n 'see': 60,\n 'been': 61,\n 'any': 62,\n 'also': 63,\n 'here': 64,\n 'because': 65,\n 'his': 66,\n 'some': 67,\n \"i'm\": 68,\n 'know': 69,\n 'people': 70,\n 'did': 71,\n 'how': 72,\n 'only': 73,\n 'why': 74,\n 'use': 75,\n \"it's\": 76,\n 'does': 77,\n 'when': 78,\n 'time': 79,\n 'discussion': 80,\n 'articles': 81,\n 'were': 82,\n 'am': 83,\n 'information': 84,\n 'them': 85,\n 'their': 86,\n 'user': 87,\n 'then': 88,\n 'want': 89,\n 'could': 90,\n 'good': 91,\n 'now': 92,\n 'thanks': 93,\n 'many': 94,\n 'edit': 95,\n 'than': 96,\n 'make': 97,\n 'even': 98,\n 'out': 99,\n 'very': 100,\n 'these': 101,\n 'may': 102,\n 'had': 103,\n 'up': 104,\n 'sources': 105,\n 'name': 106,\n 'first': 107,\n 'way': 108,\n 'well': 109,\n 'say': 110,\n 'help': 111,\n 'new': 112,\n 'wp': 113,\n 'get': 114,\n 'image': 115,\n 'pages': 116,\n 'thank': 117,\n 'source': 118,\n 'its': 119,\n 'again': 120,\n 'really': 121,\n 'since': 122,\n 'being': 123,\n 'such': 124,\n 'section': 125,\n 'editing': 126,\n 'where': 127,\n 'work': 128,\n 'used': 129,\n 'find': 130,\n 'made': 131,\n 'go': 132,\n 'much': 133,\n 'fuck': 134,\n 'same': 135,\n 'right': 136,\n 'most': 137,\n 'someone': 138,\n 'add': 139,\n 'need': 140,\n 'after': 141,\n 'before': 142,\n 'read': 143,\n 'without': 144,\n 'something': 145,\n 'deleted': 146,\n 'point': 147,\n 'look': 148,\n 'fact': 149,\n 'two': 150,\n 'wiki': 151,\n 'list': 152,\n 'deletion': 153,\n 'still': 154,\n 'seems': 155,\n 'him': 156,\n 'said': 157,\n 'take': 158,\n \"don't\": 159,\n 'hello': 160,\n 'link': 161,\n 'stop': 162,\n 'content': 163,\n '1': 164,\n '2': 165,\n 'another': 166,\n 'those': 167,\n \"that's\": 168,\n \"you're\": 169,\n 'own': 170,\n \"i've\": 171,\n 'problem': 172,\n 'http': 173,\n 'our': 174,\n 'added': 175,\n 'into': 176,\n 'utc': 177,\n 'block': 178,\n 'however': 179,\n 'too': 180,\n 'case': 181,\n 'sure': 182,\n 'must': 183,\n 'things': 184,\n 'ask': 185,\n 'us': 186,\n 'note': 187,\n 'reason': 188,\n 'vandalism': 189,\n 'welcome': 190,\n 'back': 191,\n 'never': 192,\n 'editors': 193,\n 'change': 194,\n 'comments': 195,\n 'place': 196,\n 'person': 197,\n 'history': 198,\n 'better': 199,\n 'question': 200,\n 'personal': 201,\n 'comment': 202,\n 'part': 203,\n 'under': 204,\n 'against': 205,\n 'hope': 206,\n 'over': 207,\n 'free': 208,\n 'wrong': 209,\n 'links': 210,\n 'while': 211,\n 'removed': 212,\n 'done': 213,\n 'best': 214,\n 'her': 215,\n 'blocked': 216,\n 'understand': 217,\n '•': 218,\n 'she': 219,\n 'agree': 220,\n 'believe': 221,\n 'questions': 222,\n 'last': 223,\n '3': 224,\n 'anything': 225,\n 'put': 226,\n 'com': 227,\n 'going': 228,\n 'both': 229,\n 'delete': 230,\n 'edits': 231,\n 'little': 232,\n 'policy': 233,\n 'keep': 234,\n 'world': 235,\n 'try': 236,\n 'copyright': 237,\n 'changes': 238,\n 'yes': 239,\n 'yourself': 240,\n 'already': 241,\n 'others': 242,\n 'continue': 243,\n \"i'll\": 244,\n 'give': 245,\n 'nothing': 246,\n 'feel': 247,\n 'years': 248,\n 'long': 249,\n 'different': 250,\n 'great': 251,\n 'example': 252,\n 'reliable': 253,\n 'says': 254,\n 'maybe': 255,\n 'hi': 256,\n 'using': 257,\n 'anyone': 258,\n 'reference': 259,\n 'english': 260,\n 'leave': 261,\n 'let': 262,\n 'sorry': 263,\n 'opinion': 264,\n 'references': 265,\n 'editor': 266,\n 'text': 267,\n 'found': 268,\n 'actually': 269,\n 'site': 270,\n 'probably': 271,\n 'write': 272,\n 'remove': 273,\n 'simply': 274,\n 'subject': 275,\n 'come': 276,\n 'few': 277,\n 'war': 278,\n 'instead': 279,\n 'important': 280,\n 'thing': 281,\n 'original': 282,\n 'style': 283,\n 'trying': 284,\n 'shit': 285,\n 'contributions': 286,\n 'day': 287,\n 'real': 288,\n 'called': 289,\n 'above': 290,\n 'mean': 291,\n 'ip': 292,\n 'life': 293,\n 'word': 294,\n 'consensus': 295,\n 'least': 296,\n 'enough': 297,\n 'support': 298,\n 'created': 299,\n 'non': 300,\n 'answer': 301,\n 'show': 302,\n 'between': 303,\n 'users': 304,\n 'although': 305,\n 'www': 306,\n 'tell': 307,\n 'bad': 308,\n 'view': 309,\n 'etc': 310,\n 'account': 311,\n 'message': 312,\n 'images': 313,\n 'thought': 314,\n 'fair': 315,\n 'encyclopedia': 316,\n 'guidelines': 317,\n 'topic': 318,\n 'material': 319,\n 'title': 320,\n 'website': 321,\n 'book': 322,\n 'main': 323,\n 'request': 324,\n 'correct': 325,\n 'until': 326,\n 'tag': 327,\n 'several': 328,\n 'considered': 329,\n 'clear': 330,\n 'old': 331,\n 'always': 332,\n 'off': 333,\n 'check': 334,\n 'evidence': 335,\n 'states': 336,\n 'true': 337,\n 'review': 338,\n 'state': 339,\n 'every': 340,\n 'top': 341,\n 'else': 342,\n 'through': 343,\n 'doing': 344,\n 'written': 345,\n 'might': 346,\n '5': 347,\n 'everything': 348,\n 'lot': 349,\n '4': 350,\n 'org': 351,\n 'mentioned': 352,\n 'seem': 353,\n 'adding': 354,\n 'term': 355,\n 'number': 356,\n 'edition': 357,\n 'template': 358,\n 'clearly': 359,\n 'language': 360,\n 'each': 361,\n 'post': 362,\n 'general': 363,\n 'claim': 364,\n 'criteria': 365,\n 'makes': 366,\n \"''\": 367,\n 'idea': 368,\n 'version': 369,\n 'far': 370,\n 'means': 371,\n 'oh': 372,\n 'quick': 373,\n 'administrator': 374,\n 'possible': 375,\n 'based': 376,\n 'group': 377,\n 'words': 378,\n 'three': 379,\n 'second': 380,\n 'matter': 381,\n 'making': 382,\n 'once': 383,\n 'current': 384,\n 'pov': 385,\n 'mention': 386,\n 'consider': 387,\n 'media': 388,\n 'address': 389,\n 'create': 390,\n 'hate': 391,\n 'times': 392,\n 'call': 393,\n 'c': 394,\n 'nigger': 395,\n 'year': 396,\n 'course': 397,\n 'ass': 398,\n 'completely': 399,\n 'bit': 400,\n 'given': 401,\n 'research': 402,\n 'rules': 403,\n 'attack': 404,\n 'left': 405,\n 'man': 406,\n 'facts': 407,\n 'known': 408,\n 'date': 409,\n 'kind': 410,\n 'therefore': 411,\n 'related': 412,\n 'issue': 413,\n 'though': 414,\n 'problems': 415,\n '10': 416,\n 'included': 417,\n 'sense': 418,\n 'big': 419,\n 'end': 420,\n 'seen': 421,\n 'needs': 422,\n 'following': 423,\n 'published': 424,\n 'addition': 425,\n 'statement': 426,\n 'rather': 427,\n 'according': 428,\n 'american': 429,\n 're': 430,\n 'especially': 431,\n 'previous': 432,\n 'jpg': 433,\n 'start': 434,\n 'include': 435,\n 'less': 436,\n 'down': 437,\n 'either': 438,\n 'around': 439,\n 'united': 440,\n 'en': 441,\n 'changed': 442,\n 'suggest': 443,\n 'u': 444,\n 'explain': 445,\n 'appropriate': 446,\n 'happy': 447,\n 'e': 448,\n 'file': 449,\n 'care': 450,\n 'category': 451,\n 'whether': 452,\n 'gay': 453,\n 'saying': 454,\n 'anyway': 455,\n 'mind': 456,\n 'quite': 457,\n '2005': 458,\n 'project': 459,\n 'love': 460,\n 'days': 461,\n 'next': 462,\n 'having': 463,\n 'school': 464,\n 'everyone': 465,\n 'ever': 466,\n 'four': 467,\n 'quote': 468,\n 'relevant': 469,\n 'useful': 470,\n 'interested': 471,\n 'neutral': 472,\n 'wanted': 473,\n 'news': 474,\n 'story': 475,\n 'whole': 476,\n 'got': 477,\n 'including': 478,\n 'summary': 479,\n 'community': 480,\n 'side': 481,\n 'provide': 482,\n 'later': 483,\n 'notice': 484,\n 'fucking': 485,\n 'listed': 486,\n 't': 487,\n 'wrote': 488,\n 's': 489,\n 'yet': 490,\n 'able': 491,\n 'position': 492,\n 'specific': 493,\n 'line': 494,\n 'sign': 495,\n 'remember': 496,\n 'warning': 497,\n 'suck': 498,\n 'picture': 499,\n 'attacks': 500,\n 'hey': 501,\n 'issues': 502,\n 'public': 503,\n 'discuss': 504,\n 'sentence': 505,\n 'color': 506,\n 'small': 507,\n 'interest': 508,\n '0': 509,\n 'started': 510,\n 'myself': 511,\n 'looking': 512,\n 'redirect': 513,\n 'official': 514,\n 'almost': 515,\n '6': 516,\n 'currently': 517,\n 'recent': 518,\n 'obviously': 519,\n 'names': 520,\n '2006': 521,\n 'tried': 522,\n 'during': 523,\n 'full': 524,\n 'city': 525,\n 'below': 526,\n '20': 527,\n 'anti': 528,\n 'stupid': 529,\n 'involved': 530,\n 'editions': 531,\n 'live': 532,\n 'god': 533,\n 'game': 534,\n 'country': 535,\n 'common': 536,\n 'asked': 537,\n 'similar': 538,\n 'faith': 539,\n 'stay': 540,\n '7': 541,\n 'move': 542,\n 'further': 543,\n 'process': 544,\n 'lol': 545,\n 'books': 546,\n 'edited': 547,\n 'policies': 548,\n 'report': 549,\n 'claims': 550,\n 'background': 551,\n 'certainly': 552,\n 'high': 553,\n 'pretty': 554,\n 'due': 555,\n 'notable': 556,\n 'writing': 557,\n 'reasons': 558,\n 'admin': 559,\n '24': 560,\n 'die': 561,\n 'future': 562,\n 'regarding': 563,\n 'noticed': 564,\n 'away': 565,\n \"you've\": 566,\n 'taken': 567,\n 'false': 568,\n 'political': 569,\n 'itself': 570,\n '100': 571,\n 'ago': 572,\n 'perhaps': 573,\n 'p': 574,\n 'government': 575,\n 'truth': 576,\n '11': 577,\n 'exactly': 578,\n \"doesn't\": 579,\n 'sandbox': 580,\n 'ok': 581,\n 'shows': 582,\n 'power': 583,\n 'british': 584,\n 'search': 585,\n 'working': 586,\n 'today': 587,\n 'speak': 588,\n 'system': 589,\n 'd': 590,\n 'unless': 591,\n 'came': 592,\n 'google': 593,\n 'single': 594,\n 'often': 595,\n 'necessary': 596,\n 'de': 597,\n 'party': 598,\n '8': 599,\n 'test': 600,\n 'talking': 601,\n 'behavior': 602,\n '2007': 603,\n 'follow': 604,\n '12': 605,\n 'national': 606,\n 'b': 607,\n 'entry': 608,\n 'paragraph': 609,\n 'argument': 610,\n '2008': 611,\n 'fat': 612,\n 'hard': 613,\n 'area': 614,\n 'soon': 615,\n 'points': 616,\n 'looks': 617,\n 'five': 618,\n 'law': 619,\n '000': 620,\n 'terms': 621,\n 'administrators': 622,\n 'become': 623,\n 'particular': 624,\n 'nice': 625,\n 'username': 626,\n 'improve': 627,\n 'order': 628,\n 'company': 629,\n 'within': 630,\n 'took': 631,\n 'reading': 632,\n 'appreciate': 633,\n '15': 634,\n 'idiot': 635,\n 'npov': 636,\n 'getting': 637,\n 'self': 638,\n 'music': 639,\n 'rule': 640,\n 'contact': 641,\n 'nor': 642,\n 'guy': 643,\n 'description': 644,\n 'knowledge': 645,\n 'complete': 646,\n 'white': 647,\n 'black': 648,\n 'form': 649,\n 'dispute': 650,\n 'type': 651,\n 'open': 652,\n 'explanation': 653,\n \"can't\": 654,\n 'saw': 655,\n '9': 656,\n 'learn': 657,\n 'provided': 658,\n 'obvious': 659,\n 'removing': 660,\n 'definition': 661,\n \"didn't\": 662,\n 'external': 663,\n 'avoid': 664,\n 'simple': 665,\n 'comes': 666,\n 'short': 667,\n 'revert': 668,\n 'eliminated': 669,\n 'author': 670,\n 'wikiproject': 671,\n 'past': 672,\n 'told': 673,\n 'decide': 674,\n 'remarkable': 675,\n 'works': 676,\n 'university': 677,\n 'class': 678,\n '14': 679,\n 'status': 680,\n 'internet': 681,\n 'refer': 682,\n 'band': 683,\n 'week': 684,\n 'accept': 685,\n 'recently': 686,\n \"there's\": 687,\n 'third': 688,\n 'response': 689,\n 'speedy': 690,\n 'mr': 691,\n 'certain': 692,\n 'theory': 693,\n 'generally': 694,\n 'otherwise': 695,\n 'respect': 696,\n 'interesting': 697,\n '2004': 698,\n 'moron': 699,\n 'family': 700,\n 'conflict': 701,\n 'web': 702,\n 'wish': 703,\n 'members': 704,\n 'posts': 705,\n 'box': 706,\n 'result': 707,\n 'job': 708,\n 'longer': 709,\n 'email': 710,\n 'fine': 711,\n 'series': 712,\n 'present': 713,\n '16': 714,\n 'alone': 715,\n 'attention': 716,\n \"i'd\": 717,\n 'whatever': 718,\n 'copy': 719,\n 'removal': 720,\n 'happened': 721,\n 'hours': 722,\n 'reverted': 723,\n 'bitch': 724,\n 'worked': 725,\n 'lead': 726,\n 'nonsense': 727,\n 'available': 728,\n 'jews': 729,\n 'contribute': 730,\n 'john': 731,\n 'large': 732,\n 'context': 733,\n 'placed': 734,\n 'death': 735,\n 'actions': 736,\n 'details': 737,\n \"wikipedia's\": 738,\n 'views': 739,\n '18': 740,\n 'hell': 741,\n 'needed': 742,\n 'set': 743,\n 'automatically': 744,\n 'science': 745,\n 'proposed': 746,\n '17': 747,\n 'historical': 748,\n 'allowed': 749,\n 'deleting': 750,\n 'human': 751,\n 'statements': 752,\n 'guess': 753,\n 'okay': 754,\n 'german': 755,\n 'appear': 756,\n 'sock': 757,\n 'data': 758,\n '13': 759,\n 'notability': 760,\n 'friend': 761,\n 'valid': 762,\n 'conversation': 763,\n 'indicate': 764,\n 'jewish': 765,\n '19': 766,\n 'meaning': 767,\n 'appears': 768,\n 'separate': 769,\n '2009': 770,\n 'eliminate': 771,\n 'play': 772,\n 'action': 773,\n 'usually': 774,\n 'among': 775,\n '23': 776,\n '21': 777,\n 'accepted': 778,\n 'opinions': 779,\n 'enjoy': 780,\n 'piece': 781,\n 'standard': 782,\n 'serious': 783,\n 'went': 784,\n 'banned': 785,\n 'cheers': 786,\n 'uploaded': 787,\n 'entire': 788,\n 'taking': 789,\n 'along': 790,\n 'cited': 791,\n 'level': 792,\n 'rights': 793,\n 'july': 794,\n '22': 795,\n 'online': 796,\n 'likely': 797,\n 'sex': 798,\n 'seriously': 799,\n 'march': 800,\n 'themselves': 801,\n '30': 802,\n 'posted': 803,\n 'close': 804,\n 'elimination': 805,\n 'admins': 806,\n \"let's\": 807,\n 'video': 808,\n '2010': 809,\n 'damn': 810,\n 'personally': 811,\n 'months': 812,\n 'criticism': 813,\n 'hand': 814,\n \"he's\": 815,\n 'together': 816,\n 'info': 817,\n 'freedom': 818,\n \"isn't\": 819,\n 'june': 820,\n 'directly': 821,\n 'contribs': 822,\n 'ban': 823,\n 'afd': 824,\n 'moved': 825,\n 'nobody': 826,\n 'quotes': 827,\n 'mistake': 828,\n 'quickly': 829,\n 'film': 830,\n 'sections': 831,\n 'except': 832,\n 'lack': 833,\n 'difference': 834,\n 'august': 835,\n 'south': 836,\n 'label': 837,\n 'himself': 838,\n 'uses': 839,\n 'act': 840,\n 'special': 841,\n \"'\": 842,\n 'debate': 843,\n 'access': 844,\n 'width': 845,\n 'heard': 846,\n 'm': 847,\n 'rest': 848,\n 'space': 849,\n 'exist': 850,\n 'greek': 851,\n 'situation': 852,\n 'legal': 853,\n 'blocking': 854,\n 'movie': 855,\n 'creating': 856,\n 'finally': 857,\n 'per': 858,\n 'organization': 859,\n 'proof': 860,\n 'tags': 861,\n 'church': 862,\n 'control': 863,\n 'f': 864,\n 'india': 865,\n 'controversial': 866,\n 'quality': 867,\n 'doubt': 868,\n 'team': 869,\n 'wants': 870,\n 'difficult': 871,\n 'photo': 872,\n 'messages': 873,\n 'license': 874,\n 'described': 875,\n 'scientific': 876,\n 'sucks': 877,\n 'inappropriate': 878,\n 'contribution': 879,\n 'record': 880,\n 'deal': 881,\n 'sites': 882,\n 'countries': 883,\n 'cases': 884,\n 'violation': 885,\n 'none': 886,\n 'reversed': 887,\n 'january': 888,\n 'children': 889,\n 'v': 890,\n 'goes': 891,\n 'bias': 892,\n 'unfortunately': 893,\n 'legitimate': 894,\n \"what's\": 895,\n 'items': 896,\n 'sometimes': 897,\n 'regards': 898,\n 'living': 899,\n 'prove': 900,\n 'military': 901,\n 'uk': 902,\n 'pro': 903,\n 'culture': 904,\n 'send': 905,\n 'table': 906,\n 'april': 907,\n 'discussed': 908,\n 'movement': 909,\n 'half': 910,\n 'friends': 911,\n 'fish': 912,\n '25': 913,\n 'lost': 914,\n 'return': 915,\n 'red': 916,\n 'various': 917,\n 'million': 918,\n 'html': 919,\n '50': 920,\n 'meet': 921,\n 'stuff': 922,\n 'head': 923,\n 'biased': 924,\n 'mother': 925,\n 'pictures': 926,\n 'existing': 927,\n 'abuse': 928,\n 'allow': 929,\n 'sent': 930,\n 'la': 931,\n 'gave': 932,\n 'fun': 933,\n 'w': 934,\n '—': 935,\n 'independent': 936,\n 'received': 937,\n 'decision': 938,\n 'decided': 939,\n 'wait': 940,\n 'numbers': 941,\n 'apparently': 942,\n 'modern': 943,\n 'field': 944,\n 'computer': 945,\n 'force': 946,\n 'arguments': 947,\n 'anonymous': 948,\n 'born': 949,\n '·': 950,\n 'incorrect': 951,\n 'aware': 952,\n 'guide': 953,\n 'december': 954,\n 'fix': 955,\n 'member': 956,\n 'cut': 957,\n 'assume': 958,\n 'websites': 959,\n 'september': 960,\n 'linked': 961,\n 'kill': 962,\n 'pillars': 963,\n 'x': 964,\n 'groups': 965,\n 'press': 966,\n 'purpose': 967,\n 'cite': 968,\n '27': 969,\n 'outside': 970,\n '2012': 971,\n 'home': 972,\n 'cause': 973,\n 'r': 974,\n 'bring': 975,\n 'house': 976,\n 'manual': 977,\n 'business': 978,\n 'wikipedian': 979,\n 'attempt': 980,\n 'period': 981,\n 'october': 982,\n 'moment': 983,\n 'creation': 984,\n 'song': 985,\n 'introduction': 986,\n 'penis': 987,\n 'character': 988,\n 'accurate': 989,\n 'cunt': 990,\n 'hesitate': 991,\n 'christian': 992,\n 'actual': 993,\n 'knows': 994,\n 'disagree': 995,\n 'irrelevant': 996,\n 'february': 997,\n 'readers': 998,\n 'importance': 999,\n 'international': 1000,\n ...}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33990abdc9ed8a8d98b0f325a6038b616f111f50"
      },
      "cell_type": "code",
      "source": "#Every comment in the dataset is now a sequence of integers of unequal length\nlist_tokenized_train[0]",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "[653,\n 74,\n 1,\n 231,\n 131,\n 204,\n 32,\n 626,\n 4544,\n 11994,\n 1046,\n 82,\n 723,\n 43,\n 4999,\n 18265,\n 55,\n 6579,\n 15,\n 67,\n 2717,\n 141,\n 6,\n 3018,\n 42,\n 112,\n 1147,\n 8506,\n 2772,\n 4,\n 53,\n 159,\n 273,\n 1,\n 358,\n 33,\n 1,\n 48,\n 31,\n 122,\n 68,\n 3438,\n 92,\n 3067,\n 4621,\n 2275,\n 969]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f65c4145bd1da3d01e53d0cf42721ee32bc03bb"
      },
      "cell_type": "code",
      "source": "#Using Keras' Tokenizer function on English dataset\ntokenizer1 = Tokenizer(num_words=max_features)\ntokenizer1.fit_on_texts(list(list_sentences_train1))\n\nlist_tokenized_train1 = tokenizer1.texts_to_sequences(list_sentences_train1)\nlist_tokenized_test1 = tokenizer1.texts_to_sequences(list_sentences_test)\n\nX_t1 = pad_sequences(list_tokenized_train1, maxlen=maxlen)\nX_te1 = pad_sequences(list_tokenized_test1, maxlen=maxlen)\n\nword_index1 = tokenizer.word_index\nprint('Found %s unique tokens.' %len(word_index1))\nnp.shape(X_t1)",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found 246556 unique tokens.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "(159571, 100)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fadbebd140a4773baba0f325416af3fd69220c2"
      },
      "cell_type": "code",
      "source": "#Every comment is a sequence of indices\nlist_tokenized_train1[0]",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "[688,\n 75,\n 1,\n 126,\n 130,\n 177,\n 29,\n 672,\n 4511,\n 12052,\n 1116,\n 86,\n 331,\n 51,\n 2278,\n 11448,\n 50,\n 6864,\n 15,\n 60,\n 2756,\n 148,\n 7,\n 2937,\n 34,\n 117,\n 1221,\n 15190,\n 2825,\n 4,\n 45,\n 59,\n 244,\n 1,\n 365,\n 31,\n 1,\n 38,\n 27,\n 143,\n 73,\n 3462,\n 89,\n 3085,\n 4583,\n 2273,\n 985]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "660fb0a6582049ccacf3eb8e0c8bfc0d89c234e7"
      },
      "cell_type": "markdown",
      "source": "# GloVe Word Embeddings\n\nWe will be using GloVe embeddings, which you can read about here. GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics.\n\nSpecifically, we will use the **200-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia.** \n\n## Preparing Embedding Layer\n\nReturns a 20-dimensional vector space coordinate for each word in the embedding, i.e. 400k words each have a 200-d representation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28f6b5c55c144e97ea3cba4be7d250774b5bfa04"
      },
      "cell_type": "code",
      "source": "#Dictionary of word and their embedding values\n#There are embeddings for 400k words and each embedding is a 200-d vector\nembeddings_index={}\n\nf = open(embedding_file)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\nprint('Found %s word vectors.'%len(embeddings_index))",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found 400000 word vectors.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7dac7890c32661fa3f67e18d128ac604f0cdf971"
      },
      "cell_type": "markdown",
      "source": "## Preparing Embedding Matrix\n\nWords that are not in GloVe are randomly initialized\nCreating an embedding matrix for 20000 words where each word has a 200 dimensional representation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a0259afb1f425bb306070ac3fd74c72d3026292"
      },
      "cell_type": "code",
      "source": "#Embedding matrix for 400k words\nall_embeddings = np.stack(embeddings_index.values())\n\nemb_mean= all_embeddings.mean()\nemb_stddev = all_embeddings.std()\n\nnp.shape(all_embeddings)",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  \n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "(400000, 200)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5600b6b3398d2cf592698a2b2345dd20fdf4799f"
      },
      "cell_type": "code",
      "source": "#Embedding Matrix for Augmented Dataset\n#Number of words is assigned as the minimum of either 400\nnb_words = min(max_features, len(word_index)+1)\n\n#Creating a matrix of length of the number of words we choose to represent\nembedding_matrix = np.random.normal(emb_mean, emb_stddev, (nb_words, embed_size))\n\n#For augmented data\nfor word, i in word_index.items():\n    if i>=max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nnp.shape(embedding_matrix)",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "(200000, 200)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d570498d4607499db2f651b77ce4b502e697524"
      },
      "cell_type": "code",
      "source": "#Creating a matrix of length of the number of words we choose to represent\nembedding_matrix1 = np.random.normal(emb_mean, emb_stddev, (nb_words, embed_size))\n\n#For augmented data\nfor word, i in word_index1.items():\n    if i>=max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix1[i] = embedding_vector\n\nnp.shape(embedding_matrix1)",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "(200000, 200)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89466575f4bbc3d1c80145a42a4a6d065e9b55b8"
      },
      "cell_type": "code",
      "source": "#Embedding Matrix for Augmented Dataset\n\nnb_words = min(max_features, len(word_index1)+1)\nembedding_matrix1 = np.random.normal(emb_mean, emb_stddev, (nb_words, embed_size))\nfor word, i in word_index1.items():\n    if i>=max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix1[i] = embedding_vector\n\nnp.shape(embedding_matrix)",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "(200000, 200)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e3ec14c49ed39703f69517750aabf1bacfd0f95b"
      },
      "cell_type": "markdown",
      "source": "## Model\n\nSimple bidirectional LSTM with two fully connected layers and adding some dropout is necessary."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "915a3de175c909954819f8c6a0e2ba0bf338cd8c"
      },
      "cell_type": "code",
      "source": "#Early Stopping \n\nes = keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=0,\n                              verbose=0, mode='auto')\n\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)\n        roc = roc_auc_score(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "52eb4bd20a5cca3b3864f36de89c18e015c7c2ab"
      },
      "cell_type": "code",
      "source": "#Model with Data Augmentation \n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights = [embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d12633b9efee3315368af0f69af455fd48931375"
      },
      "cell_type": "code",
      "source": "#Model without Data Augmentation \n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights = [embedding_matrix1])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel1 = Model(inputs=inp, outputs=x)\nmodel1.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "128bb27e4071575c929e71326994d25e3eced5af"
      },
      "cell_type": "code",
      "source": "#Model fitting with Augmented Data\nhistory = model.fit(X_t, y_t, batch_size=256, epochs=5, validation_split=0.2,callbacks=[es]);",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 382970 samples, validate on 95743 samples\nEpoch 1/5\n382970/382970 [==============================] - 851s 2ms/step - loss: 0.0630 - acc: 0.9784 - val_loss: 0.0446 - val_acc: 0.9832\nEpoch 2/5\n382970/382970 [==============================] - 855s 2ms/step - loss: 0.0382 - acc: 0.9852 - val_loss: 0.0365 - val_acc: 0.9860\nEpoch 3/5\n382970/382970 [==============================] - 858s 2ms/step - loss: 0.0300 - acc: 0.9882 - val_loss: 0.0332 - val_acc: 0.9874\nEpoch 4/5\n382970/382970 [==============================] - 856s 2ms/step - loss: 0.0244 - acc: 0.9905 - val_loss: 0.0306 - val_acc: 0.9890\nEpoch 5/5\n382970/382970 [==============================] - 856s 2ms/step - loss: 0.0192 - acc: 0.9927 - val_loss: 0.0296 - val_acc: 0.9901\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b640173b5244baa3826412504db43ce8c15bac4b"
      },
      "cell_type": "code",
      "source": "#Model fitting without Augmented Data\nhistory1 = model1.fit(X_t1, y_t1, batch_size=256, epochs=5, validation_split=0.2,callbacks=[es]);",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 127656 samples, validate on 31915 samples\nEpoch 1/5\n127656/127656 [==============================] - 292s 2ms/step - loss: 0.0982 - acc: 0.9717 - val_loss: 0.0540 - val_acc: 0.9807\nEpoch 2/5\n127656/127656 [==============================] - 286s 2ms/step - loss: 0.0488 - acc: 0.9822 - val_loss: 0.0493 - val_acc: 0.9822\nEpoch 3/5\n127656/127656 [==============================] - 288s 2ms/step - loss: 0.0410 - acc: 0.9845 - val_loss: 0.0505 - val_acc: 0.9823\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "397b8ccff6b59c7a6b2ff8f27b1f372da9600688"
      },
      "cell_type": "code",
      "source": "y_test = model.predict([X_te],batch_size = 1024, verbose=1)\ny_test1 = model1.predict([X_te],batch_size = 1024, verbose=1)\n\n\n'''\nsample_submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsample_submission[list_classes] = y_test\nsample_submission.to_csv('submission.csv',index = False)\n'''",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "153164/153164 [==============================] - 29s 192us/step\n153164/153164 [==============================] - 29s 192us/step\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 46,
          "data": {
            "text/plain": "\"\\nsample_submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\\nsample_submission[list_classes] = y_test\\nsample_submission.to_csv('submission.csv',index = False)\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0d4208bf597a037530b2dac0698b0edc8bb1946"
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "036cc4987988705cfe034931691dfa7fdd5ac75b"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n#print(history.history.keys())\n\n\nplt.figure(figsize=(16,12))\nplt.suptitle('Accuracy and Loss Comparison')\n\ngridspec.GridSpec(2,2)\nplt.subplot2grid((2,2),(0,0))\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Augmented Model accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train','test'],loc='upper left')\n\n\nplt.subplot2grid((2,2),(0,1))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Augmented Model loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train','test'],loc='upper left')\n\nplt.subplot2grid((2,2),(1,0))\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('Model accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train','test'],loc='upper left')\n\n\nplt.subplot2grid((2,2),(1,1))\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('Model loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train','test'],loc='upper left')\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "116915454a6f10df77227f96cbb88bb550ea534e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}